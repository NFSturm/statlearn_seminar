---
title: "Tree-based methods"
author: "Niclas Frederic Sturm"
date: "22-10-2019"
output:
  html_document:
    theme: yeti
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document contains the code for the seminar "Statistical Learning and Econometrics", with a focus on tree-based models. Four models will be used for modeling: A simple decision tree, a bagged model, boosting as well as a random forest. To accomplish a good benchmarking estimated, we will use two datasets; one for regression, the other for classification. We will first import the UCI Communities and Crime Dataset as the regression dataset and an Employee Churn dataset provided by IBM. 

```{r warning = FALSE, message = FALSE}
library(rpart) # Loading required packages
library(rpart.plot)
library(rsample)
library(dplyr)
library(readr)
library(naniar)
library(Metrics)
library(ipred)
library(purrr)
library(ranger)
library(gbm)
library(ggplot2)

set.seed(1997) # Set seed for reproducibility
setwd("/Users/nfsturm/Documents/STATLEARN/forestranger")

crime_data <- read_csv("crimedata.csv", col_names = TRUE)
drop_cols <- c("communityname", "state", "countyCode", "communityCode", "fold", "murdPerPop", "rapesPerPop", "robbbPerPop", "assaultPerPop", "burglPerPop", "larcPerPop", "autoTheftPerPop", "arsonsPerPop", "nonViolPerPop", "murders", "rapes", "robberies", "assaults", "burglaries", "larcenies", "autoTheft", "arsons")
crime_data <- crime_data %>%
  select(-drop_cols) %>%
  replace_with_na_all(condition = ~.x == "?")

drop_vars <- miss_var_summary(crime_data)
drop_vars <- drop_vars %>%
  filter(pct_miss > 10) %>%
  select(variable) %>%
  pull()

crime_data <- crime_data %>%
  select(-drop_vars)

crime_data <- drop_na(crime_data)
crime_data <- map_at(crime_data, .at = 1:103, .f = as.numeric)
crime_data <- as_tibble(crime_data)
```

We will randomly split the data into a training and testing dataset. 

```{r}
crime_splits <- initial_split(crime_data)
train <- training(crime_splits)
test <- testing(crime_splits)
```

Now, we will fit a regression tree using recursive partitioning. The response variable is "ViolentCrimesPerPop", i.e. the number of violent crimes per 100,000 inhabitants. 

```{r message=FALSE}
tree_crime <- rpart(ViolentCrimesPerPop~., method = "anova", data = train)
rpart.plot(tree_crime, branch = 0.04)
```

The regression tree results in a tree that predicts nine distinct values. Following this simple regression tree, we will estimate a model using bootstrap aggregation. 

```{r}
bag_model <- bagging(ViolentCrimesPerPop~., data = train, nbagg = 500, coob = TRUE)
```

Next, a gradient boosting machine will be used. We will the popular "gbm" package to build a model. 

```{r}
boost_model <- gbm(ViolentCrimesPerPop~., distribution = "gaussian", data = train, n.trees = 2000, shrinkage = 0.1, cv.folds = 10)
best <- which.min(boost_model$cv.error)
sqrt(boost_model$cv.error[best])
```

The best tree grown by boosting achieves a cross-validated RMSE of 383. We conclude the analysis of regression trees with a random forest. To this end we will be using the "ranger" package, a fast implementation. Again, ten-fold cross-validated will be used to evaluate the model. 

```{r}

cv_split <- vfold_cv(train, v = 10)

cv_data <- cv_split %>% # Calculating the cross-validated error for the model. 
  mutate(train = map(splits, ~training(.x)),
         validation = map(splits, ~testing(.x)))

cv_tune <- cv_data %>%
  crossing(mtry = 1:15) %>%
  mutate(model = map2(train, mtry, .f = ~ranger(formula = ViolentCrimesPerPop~.,
  data = .x, mtry = .y)))

cv_tune <- cv_tune %>%
  mutate(validation_actual = map(validation, ~.x$ViolentCrimesPerPop)) %>%
  mutate(validation_predicted = map2(model, validation, ~predict(.x, .y)$predictions)) %>%
  mutate(validation_rmse = map2_dbl(validation_actual, validation_predicted, ~rmse(actual = .x, predicted =.y)))

cv_select <- cv_tune %>%
  select(mtry, model, validation_rmse) %>%
  group_by(mtry) %>%
  summarize(mean.rmse = mean(validation_rmse))

cv_select
```

We will now plot mtry against the cross-validated RMSE metric.

```{r}
ggplot(cv_select, aes(x = mtry, y = mean.rmse)) + geom_line(col = "#7fa9c1") + geom_point(col = "#7fa9c1") + theme_classic() + labs(title = "Random Forest RMSE", subtitle = "Implemented with Ranger") + xlab("Mtry") + ylab("Cross-validated RMSE") + scale_x_continuous(breaks = seq(1, 15, 2))
```

The results show that the minimum RMSE is achieved for a number of five variables that are considered at each split (mtry = 5). 
