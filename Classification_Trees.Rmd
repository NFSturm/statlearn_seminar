---
title: "Classification Trees"
author: "Niclas Frederic Sturm"
date: "3-11-2019"
output:
  pdf_document: default
  html_document:
    theme: yeti
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Dieses Dokument enthält die Programmierbefehle zur Erstellung von Modellen auf Basis von Entscheidungsbäumen. 

```{r message = FALSE}
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(rsample)
library(Metrics)
library(ipred)
library(doParallel)
library(foreach)
library(stringr)
library(purrr)
library(tibble)
library(gbm)

set.seed(1997) # Set seed for reproducibility
setwd("/Users/nfsturm/Documents/STATLEARN/forestranger")

churn_data <- read_csv("employee_attrition.csv", col_names = TRUE)
drop_vars <- c("EmployeeCount", "EmployeeNumber", "Over18", "StandardHours", "HourlyRate", "MonthlyRate", "DailyRate")
churn_data <- churn_data %>%
  select(-drop_vars)

churn_data <- churn_data %>%
  mutate(Attrition = ifelse(Attrition == 'Yes', 1, 0))

factor_vars <- c("BusinessTravel", "Department", "EducationField", "Gender", "JobRole", "MaritalStatus", "OverTime")

churn_data[factor_vars] <- lapply(churn_data[factor_vars], as.factor)

```

Auch für Klassifikationsaufgaben entspricht das *Split-Train-Test*-Paradigma gegenwärtigen Standards. 

```{r}
churn_splits <- initial_split(churn_data)
test <- testing(churn_splits)
train <- training(churn_splits)
```

## Decision Trees

Die abhängige Variable - »Attrition« - wird nun zur Zielvariable eines einfachen Entscheidungsbaums. 

```{r}
tree_churn <- rpart(Attrition ~., data = train, method = "class")
```

Eine graphische Darstellung des Entscheidungsbaums zeigt, dass der Baum 13 verschiedene Kriterien zur Teilung verwendet. 

```{r}
rpart.plot(tree_churn, branch = 0.5, tweak = 1.5)
```

## Bagging 

```{r}
nbagg_list = c(seq(1, 500, 25))

eval_df <- train %>%
  crossing(nbagg = nbagg_list) %>%
  group_by(nbagg) %>%
  nest(.key = "train") %>%
  mutate(model = map2(train, nbagg, .f = ~bagging(formula = Attrition~., data = .x, nbagg = .y)))

test_nested <- nest(test)
test_nested <- test_nested %>%
  crossing(1:length(nbagg_list)) %>%
  select(data)

colnames(test_nested) <- "test"

customRound <- function(x) {
  rounded <- ifelse(x > 0.5, 1, 0)
  return(rounded)
} 
"Um die Unsicherheit der Schätzung abzudecken, kann über diese Funktion
ein Grenzwert gewählt werden, ab dem eine binäre Schätzung vorgenommen wird."

bound_df <- bind_cols(eval_df, test_nested)
bound_df <- bound_df %>%
  mutate(test_actual = map(test, ~.x$Attrition)) %>%
  mutate(test_predicted = map2(model, test, ~predict(.x, .y))) %>%
  mutate(test_predicted = map(test_predicted, ~customRound(.x))) %>%
  mutate(test_accuracy = map2_dbl(test_actual, test_predicted, ~accuracy(actual = .x, predicted = .y)))

nbagg_tbl <- enframe(nbagg_list, value = "nbagg", name = NULL)
accuracies_tbl <- enframe(bound_df$test_accuracy, value = "accuracy", name = NULL)
accuracies_df <- bind_cols(nbagg_tbl, accuracies_tbl)
```

Die Entwicklung des Genauigkeits-Maßs kann grafisch dargestellt werden. 

```{r}
ggplot(accuracies_df, aes(nbagg, accuracy)) + geom_point() + geom_line() + theme_classic() + 
  labs(title = "Genauigkeitsmaß nach Anzahl der Bootstrap-Replikationen", caption = "Verwendung 20 diskreter Werte") + 
  xlab("Anzahl der aggregierten Bäume") + ylab("Genauigkeit") + scale_y_continuous(limits = c(0.8, 0.86)) + scale_x_continuous(limits = c(0, 500))
```

## Boosting

```{r}
boost_model <- gbm(Attrition~., distribution = "gaussian", data = train, n.trees = 1000, shrinkage = 0.1, cv.folds = 10, n.minobsinnode = 10)
predicted <- predict(boost_model, newdata = test, n.trees = 1000) 
predicted <- ifelse(predicted > 0.5, 1, 0)
actual <- test$Attrition

```


## Random Forest

