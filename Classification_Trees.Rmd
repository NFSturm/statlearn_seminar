---
title: "Classification Trees"
author: "Niclas Frederic Sturm"
date: "3-11-2019"
output:
  pdf_document: default
  html_document:
    theme: yeti
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Dieses Dokument enthält die Programmierbefehle zur Erstellung von Modellen auf Basis von Entscheidungsbäumen. 

```{r message = FALSE}
library(readr)
library(dplyr)
library(rpart)
library(rpart.plot)
library(rsample)
library(Metrics)
library(ipred)
library(doParallel)
library(foreach)
library(stringr)

set.seed(1997) # Set seed for reproducibility
setwd("/Users/nfsturm/Documents/STATLEARN/forestranger")

churn_data <- read_csv("employee_attrition.csv", col_names = TRUE)
drop_vars <- c("EmployeeCount", "EmployeeNumber")
churn_data <- churn_data %>%
  select(-drop_vars)

churn_data <- churn_data %>%
  mutate(Attrition = ifelse(Attrition == 'Yes', 1, 0))
```

Auch für Klassifikationsaufgaben entspricht das *Split-Train-Test*-Paradigma gegenwärtigen Standards. 

```{r}
churn_splits <- initial_split(churn_data)
test <- testing(churn_splits)
train <- training(churn_splits)
```

## Decision Trees

Die abhängige Variable - »Attrition« - wird nun zur Zielvariable eines einfachen Entscheidungsbaums. 

```{r}
tree_churn <- rpart(Attrition ~., data = train, method = "class")
```

Eine graphische Darstellung des Entscheidungsbaums zeigt, dass der Baum 13 verschiedene Kriterien zur Teilung verwendet. 

```{r}
rpart.plot(tree_churn, branch = 0.5, tweak = 1.5)
```

## Bagging 

```{r}
bagg_model <- bagging(Attrition~., data = train, nbagg = 100)
bagg_prob <- predict(bagg_model, test)
actual <- test$Attrition
bagg_predictions <- ifelse(bagg_prob > 0.5, 1, 0)

```


## Boosting

## Random Forest

