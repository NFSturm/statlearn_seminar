---
title: "Regression Trees"
author: "Niclas Frederic Sturm"
date: "22-10-2019"
output:
  html_document:
    theme: yeti
  pdf_document: default
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Dieses Dokument enthält die Programmierbefehle zur Erstellung von Modellen auf Basis von Entscheidungsbäumen. Die Grundlage für die hier verwendeten Modelle sind einfache Entscheidungsbäume. Darauf aufbauend werden die »Ensemble-Lerner« erstellt. Zu diesen zählen solche Modelle, die mithilfe von »Bootstrap Aggregation« *parallel* trainiert wurden bzw. mithilfe von »Boosting« eine *tiefe* Struktur aufweisen. Die Anwendungsbeispiele in diesem Dokument beschränken sich auf Regressionsbäume, d.h. solche Bäume, deren Zielvariable stetig ist. Hierzu wurde der »UCI Crime and Communities«-Datensatz verwendet.

```{r warning = FALSE, message = FALSE}
library(rpart) # Import der notwendigen Installationen
library(rpart.plot)
library(rsample)
library(dplyr)
library(readr)
library(naniar)
library(Metrics)
library(ipred)
library(purrr)
library(ranger)
library(gbm)
library(ggplot2)
library(doParallel)
library(foreach)
library(tidyr)
library(stringr)
library(caret)

set.seed(1997) # Zur Gewährleistung von Replizierbarkeit wird im Hintergrund ein Zufallswert gesetzt
setwd("/Users/nfsturm/Documents/STATLEARN/forestranger")

crime_data <- read_csv("crimedata.csv", col_names = TRUE)
drop_cols <- c("communityname", "state", "countyCode", "communityCode", "fold", "murdPerPop", "rapesPerPop", "robbbPerPop", "assaultPerPop", "burglPerPop", "larcPerPop", "autoTheftPerPop", "arsonsPerPop", "nonViolPerPop", "murders", "rapes", "robberies", "assaults", "burglaries", "larcenies", "autoTheft", "arsons")
crime_data <- crime_data %>%
  select(-drop_cols) %>%
  replace_with_na_all(condition = ~.x == "?")

drop_vars <- miss_var_summary(crime_data)
drop_vars <- drop_vars %>%
  filter(pct_miss > 10) %>%
  select(variable) %>%
  pull()

crime_data <- crime_data %>%
  select(-drop_vars)

crime_data <- drop_na(crime_data)
crime_data <- map_at(crime_data, .at = 1:103, .f = as.numeric)
crime_data <- as_tibble(crime_data)
```

Die Modellierungsstrategie folgt einem *Split-Train-Test*-Paradigma. 

```{r}
crime_splits <- initial_split(crime_data)
train <- training(crime_splits)
test <- testing(crime_splits)
```

Ein einfacher Regressionsbaum wird nun über »recursive partitioning« erstellt. Dieser Begriff bedeutet zunächst, dass ein Datensatz mit *p* erklärenden Variablen binär aufgeteilt wird, d.h. eine Variable wird herangezogen, um ein Trennungskriterium festzulegen. Im vorliegenden Beispiel ist die abhängige Variable »ViolentCrimesPerPop«, die eine Darstellung für die Zahl von Gewaltverbrechen pro 100,000 Einwohner ist. 

## Decision Trees

```{r message=FALSE}
tree_crime <- rpart(ViolentCrimesPerPop~., method = "anova", data = train, control = rpart.control(xval = 10))
rpart.plot(tree_crime, branch = 0.04, tweak = 1.2)
```

Der Regressionsbaum erzeugt neun unterschiedliche Vorhersagen. Wir können nun betrachten, wie die Wahl des Komplexitätsparameters $\alpha$ den relativen Fehler ($1-R^2$) beeinflusst. 

```{r}
plotcp(tree_crime)
```

## Bootstrap aggregating

Die »Bootstrap«-Aggregation ist ein paralleler Prozess, der sich gut nachvollziehen lässt. Die Vorgehensweise besteht darin, zunächst aus dem Training-Datensatz *k*-mal *n* Werte mit Wiederholung zu ziehen und für jeden der *k* »neuen« Datensätze einen Entscheidungsbaum zu erstellen. 

```{r}
nr_cores <- detectCores() - 2
cl <- makeCluster(nr_cores) # Verwendung der Kernzahl minus 2
registerDoParallel(cl) # Aktivierung des Parallel-Computing-Backends

# Parallele Erstellung der Entscheidungsbäume 
predictions <- foreach(
  icount(200), 
  .packages = "rpart", 
  .combine = cbind
   ) %dopar% {
    # Bootstrap-Kopie des originalen Training-Datensatzes
    index <- sample(nrow(train), replace = TRUE)
    train_boot <- train[index, ]  
  
    # Erstellen des Entscheidungsbaums auf Basis der Bootstrap-Kopie
    bagged_tree <- rpart(
      ViolentCrimesPerPop ~ ., 
      control = rpart.control(minsplit = 2, cp = 0.05),
      data = train_boot
      ) 
    
    predict(bagged_tree, newdata = test)
   }
# stopCluster(cl)

predictions <- as_tibble(predictions)

predictions_df <- predictions %>%
  mutate(instance = 1:n(), actual = test$ViolentCrimesPerPop)
  
predictions_df2 <- gather(predictions_df, nr_tree, predicted, -c(instance, actual)) %>%
  mutate(nr_tree = str_extract(nr_tree, '\\d+'))

predictions_df2$nr_tree <- as.numeric(predictions_df2$nr_tree)

tree_prep <- predictions_df2 %>%
  arrange(instance, nr_tree) %>%
  group_by(instance) %>%
  mutate(avg_prediction = cummean(predicted)) %>%
  group_by(nr_tree) %>%
  summarize(RMSE = rmse(actual, avg_prediction))
  
```

Die Evaluation der Anzahl der verwendeten Bäume lässt sich gut grafisch veranschaulichen. 

```{r}
ggplot(tree_prep, aes(nr_tree, RMSE)) + geom_line(col = "#7fa9c1", size = 2) + theme_classic() + theme(text = element_text(size=20)) + labs(title = "RMSE-Maß für Bootstrapping") + xlab("Anzahl der Bootstrap-Replikationen") + ylab("Test RMSE")
```

## Boosting

Anders als der »Bagging«-Algorithmus versucht der »Boosting«-Algorithmus *sequentiell* aus schwachen Lernen stärkere zu erzeugen. Damit fokussiert er deutlich stärker als die Bagging-Methode auf falsch klassifizierten Beobachtungen bzw. solche Beobachtungen, die ein großes Residuum auf Basis des vorherigen Modells erzeugen. 

```{r}
boost_model <- gbm(ViolentCrimesPerPop~., distribution = "gaussian", data = train, n.trees = 2000, shrinkage = 0.1, cv.folds = 10)
best <- which.min(boost_model$cv.error)
sqrt(boost_model$cv.error[best])
```

Das beste Modell erreicht einen RMSE-Wert von etwa 388. 

## Random Forests

Zuletzt 

```{r}

cv_split <- vfold_cv(train, v = 10)

cv_data <- cv_split %>%
  mutate(train = map(splits, ~training(.x)),
         validation = map(splits, ~testing(.x)))

cv_tune <- cv_data %>%
  crossing(mtry = 1:15) %>%
  mutate(model = map2(train, mtry, .f = ~ranger(formula = ViolentCrimesPerPop~.,
  data = .x, mtry = .y)))

cv_tune <- cv_tune %>%
  mutate(validation_actual = map(validation, ~.x$ViolentCrimesPerPop)) %>%
  mutate(validation_predicted = map2(model, validation, ~predict(.x, .y)$predictions)) %>%
  mutate(validation_rmse = map2_dbl(validation_actual, validation_predicted, 
                                    ~rmse(actual = .x, predicted =.y)))

cv_select <- cv_tune %>%
  select(mtry, model, validation_rmse) %>%
  group_by(mtry) %>%
  summarize(mean.rmse = mean(validation_rmse))

cv_select
```

Für mtry = 5 erreicht der Random Forest den geringsten Kreuzvalidierungsfehler. 

```{r}
ranger_model <- ranger(ViolentCrimesPerPop~., data = train, mtry = 5)
actual <- test$ViolentCrimesPerPop
predicted <- predict(ranger_model, test)
predicted <- predicted$predictions
rmse(actual, predicted)
```

Der RMSE als Funktion von mtry lässt sich gut veranschaulichen. 

```{r}
ggplot(cv_select, aes(x = mtry, y = mean.rmse)) + geom_line(col = "#7fa9c1", size = 2) + geom_point(col = "#7fa9c1", size = 3) + theme_classic() + theme(text = element_text(size=20)) + labs(title = "Random Forest RMSE") + xlab("Mtry") + ylab("Cross-validated RMSE") + scale_x_continuous(breaks = seq(1, 15, 2))
```
